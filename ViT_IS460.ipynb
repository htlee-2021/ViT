{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffaefed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.multiprocessing as mp\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec7fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PatchEmbedding class\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, \n",
    "                                  kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)  # (B, E, H', W')\n",
    "        x = rearrange(x, 'b e h w -> b (h w) e')  # (B, N, E)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affdc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VideoViT model\n",
    "class VideoViT(nn.Module):\n",
    "    def __init__(self, num_frames=10, image_size=224, patch_size=16, in_channels=3, \n",
    "                 num_classes=2, embed_dim=768, depth=12, num_heads=12, \n",
    "                 mlp_ratio=4., dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_frames = num_frames\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Calculate number of patches\n",
    "        self.num_patches_per_frame = (image_size // patch_size) ** 2\n",
    "        self.total_patches = self.num_patches_per_frame * num_frames\n",
    "        \n",
    "        # Patch Embedding\n",
    "        self.patch_embed = PatchEmbedding(image_size, patch_size, \n",
    "                                        in_channels, embed_dim)\n",
    "        \n",
    "        # Class token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Position embeddings - now accounting for temporal dimension\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, self.total_patches + 1, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Temporal embedding\n",
    "        self.temporal_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_frames, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        \n",
    "        # MLP Head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.trunc_normal_(self.temporal_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_frames, channels, height, width)\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Reshape for patch embedding\n",
    "        x = rearrange(x, 'b t c h w -> (b t) c h w')\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Reshape to separate batch and temporal dimensions\n",
    "        x = rearrange(x, '(b t) n e -> b (t n) e', b=B, t=self.num_frames)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Add temporal information\n",
    "        x_nocls = x[:, 1:, :]\n",
    "        x_nocls = rearrange(x_nocls, 'b (t n) e -> b t n e', t=self.num_frames)\n",
    "        x_nocls = x_nocls + self.temporal_embed.unsqueeze(2)\n",
    "        x_nocls = rearrange(x_nocls, 'b t n e -> b (t n) e')\n",
    "        \n",
    "        # Recombine with CLS token\n",
    "        x = torch.cat((x[:, :1, :], x_nocls), dim=1)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # MLP head (use [CLS] token)\n",
    "        x = x[:, 0]\n",
    "        x = self.mlp_head(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0255872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Dataset class\n",
    "class DFDCFrameDataset(Dataset):\n",
    "    def __init__(self, frames_root_dir, metadata_path, num_frames=10, \n",
    "                 image_size=224, transform=None, mode='train'):\n",
    "        self.frames_root_dir = frames_root_dir\n",
    "        self.num_frames = num_frames\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "            \n",
    "        # Get list of frame folders and labels\n",
    "        self.frame_folders = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Walk through the frames directory\n",
    "        for folder in os.listdir(frames_root_dir):\n",
    "            video_filename = folder.replace('_frame', '.mp4')\n",
    "            \n",
    "            if video_filename in self.metadata:\n",
    "                self.frame_folders.append(folder)\n",
    "                self.labels.append(1 if self.metadata[video_filename]['label'] == 'FAKE' else 0)\n",
    "        \n",
    "        # Default transforms if none provided\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_folders)\n",
    "    \n",
    "    def load_frames(self, frame_folder):\n",
    "        frame_paths = sorted([\n",
    "            os.path.join(self.frames_root_dir, frame_folder, f)\n",
    "            for f in os.listdir(os.path.join(self.frames_root_dir, frame_folder))\n",
    "            if f.endswith(('.jpg', '.png', '.jpeg'))\n",
    "        ])\n",
    "        \n",
    "        assert len(frame_paths) == self.num_frames, \\\n",
    "            f\"Expected {self.num_frames} frames, found {len(frame_paths)} in {frame_folder}\"\n",
    "        \n",
    "        frames = []\n",
    "        for frame_path in frame_paths:\n",
    "            try:\n",
    "                frame = Image.open(frame_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    frame = self.transform(frame)\n",
    "                frames.append(frame)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading frame {frame_path}: {str(e)}\")\n",
    "                frames.append(torch.zeros((3, 224, 224)))\n",
    "        \n",
    "        return torch.stack(frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_folder = self.frame_folders[idx]\n",
    "        label = self.labels[idx]\n",
    "        frames = self.load_frames(frame_folder)\n",
    "        return frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbfee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation functions\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(val_loader), 100. * correct / total\n",
    "\n",
    "# Create dataloaders function\n",
    "def create_frame_dataloaders(frames_root_dir, metadata_path, batch_size=8, \n",
    "                           num_frames=10, image_size=224, num_workers=4, \n",
    "                           train_split=0.8):\n",
    "    # Create dataset with training augmentations\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, \n",
    "                                saturation=0.1, hue=0.1)\n",
    "        ], p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    full_dataset = DFDCFrameDataset(\n",
    "        frames_root_dir=frames_root_dir,\n",
    "        metadata_path=metadata_path,\n",
    "        num_frames=num_frames,\n",
    "        image_size=image_size,\n",
    "        transform=train_transform,\n",
    "        mode='train'\n",
    "    )\n",
    "    \n",
    "    train_size = int(train_split * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    val_dataset.dataset.transform = val_transform\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2e231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training function\n",
    "def train_vit(train_loader, val_loader, num_classes, num_epochs=100):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = VideoViT(\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        num_classes=num_classes,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.05)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch [{epoch+1}/{num_epochs}]')\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, \n",
    "                                          optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_vit_model.pth')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage (you can modify these paths according to your setup)\n",
    "if __name__ == '__main__':\n",
    "    # Set your paths\n",
    "    FRAMES_ROOT_DIR = 'dfdc_train_part_0_balanced_1_frame'\n",
    "    METADATA_PATH = 'dfdc_train_part_0_balanced_1_frame/metadata.json'\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader = create_frame_dataloaders(\n",
    "        frames_root_dir=FRAMES_ROOT_DIR,\n",
    "        metadata_path=METADATA_PATH,\n",
    "        batch_size=8,\n",
    "        num_frames=10,\n",
    "        image_size=224\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model = train_vit(train_loader, val_loader, num_classes=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
